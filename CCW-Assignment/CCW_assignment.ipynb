{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b208d4-57c3-498d-8a40-441c6b41646d",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "The task is to build and train a classifier given a labeled dataset and then use it to infer the labels of a given unlabeled evaluation dataset. \n",
    "\n",
    "You will find the training and evaluation data on canvas.\n",
    "\n",
    "Here's the training data: TrainOnMe-2.csv \n",
    "\n",
    "Here's the evaluation data: EvaluateOnMe-2.csv \n",
    "\n",
    "Here's the ground truth: EvaluationGT-2.csv\n",
    "\n",
    "You can use whatever python libraries you like! The steps below are suggestions, but feel free to try any other techniques we discussed in class.\n",
    "\n",
    "You can submit the predicted labels by uploading them in csv format, which will then be compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3cf9a5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# For min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Some models you can try\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Packages I am importing:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aeca27-8629-4941-86c1-2d78c66ac582",
   "metadata": {},
   "source": [
    "## Load the training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25826ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "df = pd.read_csv('TrainOnMe-2.csv')\n",
    "eval_df = pd.read_csv('EvaluateOnMe-2.csv')\n",
    "\n",
    "# Split your training dataset into features and labels\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y'].apply(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc8de1-a994-4756-9989-d3f4b0de1f0d",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c95776a-27af-4777-8128-a1e9abf94a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some data pre-processing\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "dunno_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in dunno_numeric_cols:\n",
    "    converted = pd.to_numeric(X[col], errors='coerce')\n",
    "    if not converted.isna().all():  # Only convert if we get valid numbers\n",
    "        X[col] = converted\n",
    "        numeric_cols.append(col)\n",
    "\n",
    "# Check the dtypes of all features\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Convert text columns to category\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X[numeric_cols] = numeric_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "X[categorical_cols] = X[categorical_cols].astype('category')\n",
    "\n",
    "# Remove NA values and noise\n",
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Change categories to encoded labels using LabelEncoder()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# encoded_features = encoder.fit_transform(X[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad60bf-a533-4340-aabb-3d7a5c466e61",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "## Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original size of 1004, filtered to 759\n"
     ]
    }
   ],
   "source": [
    "# Try to remove outliers from training data to improve performance\n",
    "# There are different ways to do this but one way could be to use stats.zscore\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "for i, col1 in enumerate(numeric_cols):\n",
    "    for col2 in numeric_cols[i+1:]:\n",
    "        X[f'{col1}_x_{col2}'] = X[col1] * X[col2]\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "z_scores = stats.zscore(X[numeric_cols])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "\n",
    "# I selected 3 standard devs:\n",
    "threshold = 3\n",
    "\n",
    "# Filtering w/ |Z-score| < 3:\n",
    "filter_mask = (abs_z_scores < threshold).all(axis=1)\n",
    "X_filtered = X[filter_mask]\n",
    "y_filtered = y[filter_mask]\n",
    "\n",
    "print(f\"The original size of {len(X)}, filtered to {len(X_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933a70d-7f5c-46cf-b278-536053c42322",
   "metadata": {},
   "source": [
    "## Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75231986-c042-4e31-958d-7f42a3d4a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging:\n",
    "##print(y.unique())\n",
    "\n",
    "y = pd.to_numeric(y, errors='ignore')\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc043a39-d446-4720-b575-83c32fe8103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.interaction_pairs = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.interaction_pairs = [(col1, col2) for i, col1 in enumerate(numeric_cols)\n",
    "                                 for col2 in numeric_cols[i+1:]]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        interaction_features = {}\n",
    "\n",
    "        for col1, col2 in self.interaction_pairs:\n",
    "            new_feature_name = f'{col1}_x_{col2}'\n",
    "            if new_feature_name in X.columns:\n",
    "                new_feature_name += \"_dup\"  # Add a suffix if the column already exists\n",
    "            interaction_features[new_feature_name] = X[col1] * X[col2]\n",
    "\n",
    "        X = pd.concat([X, pd.DataFrame(interaction_features, index=X.index)], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5a446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoScaling: 0.486 ± 0.057\n",
      "RobustScaler: 0.486 ± 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (903). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (903). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (903). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (903). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (904). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantileTransformer: 0.514 ± 0.055\n",
      "MinMaxScaler: 0.486 ± 0.057\n",
      "StandardScaler: 0.486 ± 0.057\n",
      "\n",
      "The best one is QuantileTransformer and I got a 0.5140 accuracy.\n",
      "Best parameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 300, 'feature_selector__k': 10}\n",
      "Tuned accuracy: 0.516\n"
     ]
    }
   ],
   "source": [
    "# Scale your features\n",
    "# You can try both standardscaler and minmaxscaler and see which works better\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, \n",
    "                        module=\"sklearn.model_selection._split\")\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "\n",
    "scalers = {\n",
    "    'NoScaling': None,\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'QuantileTransformer': QuantileTransformer(output_distribution='normal'),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'StandardScaler': StandardScaler()}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "best_score = -np.inf\n",
    "best_scaler = None\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    num_transformer = scaler if scaler else 'passthrough'\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', num_transformer, numeric_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selector', SelectKBest(k=10)),  # Keep top 10 features\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ))])\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv_strategy, scoring='accuracy')\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    print(f\"{scaler_name}: {avg_score:.3f} ± {std_score:.3f}\")\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_scaler = scaler_name\n",
    "\n",
    "print(f\"\\nThe best one is {best_scaler} and I got a {best_score:.4f} accuracy.\")\n",
    "\n",
    "# Will use hyperparameter tuning:\n",
    "\n",
    "if best_scaler:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [200, 300],\n",
    "        'classifier__max_depth': [5, 10, None],\n",
    "        'feature_selector__k': [5, 10, 15]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Tuned accuracy: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7884b07e-6c9e-4d1a-942b-c18154f29010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use QuantileTransformer:\n",
    "\n",
    "# The best one is QuantileTransformer and I got a 51% (± 4.4%) accuracy.\n",
    "# HP Tuning got me 52.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f9ee0-f87c-4750-a328-dbdc9071fb07",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62b35ae-3de7-4044-a59f-831a519ba36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could try to apply SelectKBest class to extract the most useful features (this is optional but MIGHT improve accuracy)\n",
    "# Remove whichever features that are not useful\n",
    "\n",
    "# Will try this since I can't get more than 53%..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1dcc6-5f53-478d-9660-95368b4db961",
   "metadata": {},
   "source": [
    "## Split your data to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed4f88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state = 0)\n",
    "\n",
    "\n",
    "X_train.shape[0]\n",
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84cbcb0-c8ff-4d43-ab10-6c1e7d3aa0be",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "* You can try models other than the models listed below\n",
    "* You can try different hyperparameters\n",
    "* Evaluate your model using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68ffb0a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49504950495049505\n",
      "[0.46961326 0.44751381 0.45856354 0.41666667 0.50555556]\n"
     ]
    }
   ],
   "source": [
    "# Try linear SVM classifier\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Use the same preprocessor from earlier\n",
    "    ('classifier', SVC(kernel='linear', C=0.5))])\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "test_score = svm_pipeline.score(X_test, y_test)\n",
    "print(test_score)\n",
    "\n",
    "scores = cross_val_score(svm_pipeline, X_train, y_train, cv=5)\n",
    "print(scores)\n",
    "\n",
    "# linear = SVC(kernel='linear', C=0.5).fit(X_train, y_train)\n",
    "# print(linear.score(X_test,y_test))\n",
    "# # Evaluate using cross-validation\n",
    "# scores = cross_val_score(linear,X_test,y_test,cv=5)\n",
    "# print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53115ce",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42574257425742573"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try decision tree classifier\n",
    "decision_tree_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='gini', random_state=0))\n",
    "])\n",
    "\n",
    "decision_tree = decision_tree_pipeline.fit(X_train, y_train)\n",
    "dttest_score = decision_tree_pipeline.score(X_test, y_test)\n",
    "dttest_score\n",
    "\n",
    "# decision_tree = DecisionTreeClassifier(criterion = \"gini\").fit(X_train, y_train)\n",
    "# print(decision_tree.score(X_test,y_test))\n",
    "# # Evaluate using cross-validation\n",
    "# scores = cross_val_score(decision_tree,X_test,y_test,cv=10)\n",
    "# print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01507d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "randomforest_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=300, random_state=0, min_samples_split=5, max_features='sqrt'))])\n",
    "\n",
    "randomforest_pipeline.fit(X_train, y_train)\n",
    "randomforest_score = randomforest_pipeline.score(X_test, y_test)\n",
    "randomforest_score\n",
    "\n",
    "scores = cross_val_score(randomforest_pipeline, X_train, y_train, cv=10)\n",
    "\n",
    "# #Try random forest classifier\n",
    "# random_forest = RandomForestClassifier().fit(X_train, y_train)\n",
    "# print(random_forest.score(X_test,y_test))\n",
    "# scores = cross_val_score(random_forest,X_test,y_test,cv=10)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecd0b89-dd1d-45fd-a414-3f615002b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.48 (±0.05)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(randomforest_pipeline, X, y, cv=10, scoring='accuracy')\n",
    "print(f\"Cross-val accuracy: {scores.mean():.2f} (±{scores.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6059868-002a-46d8-87b9-9480cfe6efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy with feature selection: 0.51 (±0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/homl3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.51496517        nan 0.506             nan 0.51993532        nan\n",
      " 0.48008955        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'feature_selector__k': 20, 'feature_selector__score_func': <function mutual_info_classif at 0x7fffd22fa0e0>}\n",
      "Best accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Using the suggested Select K Best:\n",
    "\n",
    "best_fit_randomf = Pipeline(steps=[\n",
    "    ('interactions', InteractionCreator()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', SelectKBest(score_func=mutual_info_classif, k=15)),  # Keep top 15 features\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=300, \n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=0,\n",
    "        min_samples_split=5,\n",
    "        max_features='sqrt'\n",
    "    ))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(best_fit_randomf, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-val accuracy with feature selection: {scores.mean():.2f} (±{scores.std():.2f})\")\n",
    "\n",
    "param_grid = {\n",
    "    'feature_selector__k': [10, 15, 20, 'all'],\n",
    "    'feature_selector__score_func': [mutual_info_classif, chi2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(best_fit_randomf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f9ebf6-ea02-49fb-9e7a-5fa0fc24a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 96\n"
     ]
    }
   ],
   "source": [
    "def verify_features(pipe, X, eval_df):\n",
    "    pipe.fit(X, y)\n",
    "    \n",
    "    # Access steps using SAME NAMES as pipeline definition\n",
    "    interaction_step = pipe.named_steps['interactions']  # Now exists\n",
    "    preprocessor_step = pipe.named_steps['preprocessor']\n",
    "    \n",
    "    # Get transformed features\n",
    "    X_trans = interaction_step.transform(X)\n",
    "    X_trans = preprocessor_step.transform(X_trans)\n",
    "    \n",
    "    print(f\"Final feature count: {X_trans.shape[1]}\")\n",
    "    return X_trans\n",
    "\n",
    "# Execute\n",
    "_ = verify_features(best_fit_randomf, X, eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c44956f1-6df3-47bf-9ce0-f1d65b9893aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment, my Random Forest prediction looks better\n",
    "# amongst the others. 57% accuracy in the best case (52%).\n",
    "\n",
    "# Update: Now 58% best case (54% normal).\n",
    "# Best cross-val accuracy I got: 53%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f4a548-b8bc-4275-9da0-591ca650097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df.reindex(columns=X.columns, fill_value=0)\n",
    "best_fit_randomf.fit(X, y)\n",
    "\n",
    "train_transformed = best_fit_randomf[:-1].transform(X)\n",
    "eval_transformed = best_fit_randomf[:-1].transform(eval_df)\n",
    "\n",
    "train_features = pd.DataFrame(train_transformed, columns=[f'feature_{i}' for i in range(train_transformed.shape[1])])\n",
    "eval_features = pd.DataFrame(eval_transformed, columns=[f'feature_{i}' for i in range(eval_transformed.shape[1])])\n",
    "\n",
    "eval_predictions = best_fit_randomf.predict(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de7c8d07-e1d1-483b-9460-8843e2f220c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03157549, -0.663927  ,  1.85067221, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157523, -1.20924518, -0.3563153 , ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157532,  0.81407278, -0.48656889, ..., -0.03157545,\n",
       "         2.27243253, -0.03157545],\n",
       "       ...,\n",
       "       [ 0.03157554, -0.85182855,  0.86976355, ..., -0.03157545,\n",
       "         2.60132795, -0.03157545],\n",
       "       [ 0.0315755 , -1.19434517, -0.74480325, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157551, -1.64024435,  1.36251227, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0dc125-d773-40ef-b703-48065af7365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03157545, -1.39835781,  1.46497449, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157559, -1.72942418,  0.08136806, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157543, -1.25222032, -1.99458977, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       ...,\n",
       "       [ 0.03157553, -0.92801675, -1.60230145, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.03157534,  0.82144939, -0.13044262, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545],\n",
       "       [ 0.0315753 , -0.89983152,  2.07477097, ..., -0.03157545,\n",
       "        -0.01612756, -0.03157545]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b68b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "# Use your best model to predict the labels for the evaluation set\n",
    "\n",
    "y_pred = best_fit_randomf.predict(eval_df)\n",
    "# y_pred_final = [val for val in y_pred if val in label_map] # removed error '5' key. (No need anymore).\n",
    "\n",
    "print(set(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44895600-9cd0-4df0-920d-e18bba61162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final processing before saving. Was not getting keys but the actual values:\n",
    "\n",
    "label_map = {\n",
    "    0: 'Jorg',\n",
    "    1: 'Bob',\n",
    "    2: 'Shoogee',\n",
    "    3: 'Atsuto',\n",
    "    5: 'ERROR'\n",
    "}\n",
    "\n",
    "y_pred_filtered = [val for val in y_pred if val in label_map]\n",
    "# names_only = [label_map[val] for val in y_pred_filtered]\n",
    "names_only = [label_map.get(val, 'Unknown') for val in y_pred]\n",
    "\n",
    "predictions_df = pd.DataFrame(names_only, columns=['y'])\n",
    "predictions_df.to_csv(\"ccw_assignment_results.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6eb50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your predictions to a .csv and upload it to canvas\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"ccw_assignment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc10733-7e9e-4b76-8795-ae4a9d3f60a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
